<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Challenges | Vision-based Assistants in the Real-World </title> <meta name="author" content="VAR "> <meta name="description" content="VAR Workshop @ CVPR 2025. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/2025/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/2025/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/2025/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/2025/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2025/assets/img/icon.png?4c4bbc383b800ef80954ff10456a6a06"> <link rel="stylesheet" href="/2025/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://varworkshop.github.io/2025/challenges/"> <script src="/2025/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/2025/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2025/"> Vision-based Assistants in the Real-World </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2025/">Home </a> </li> <li class="nav-item "> <a class="nav-link" href="/2025/schedule/">Program and Papers </a> </li> <li class="nav-item "> <a class="nav-link" href="/2025/calls/">Calls </a> </li> <li class="nav-item active"> <a class="nav-link" href="/2025/challenges/">Challenges <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/2025/organizers/">Organizers </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Challenges</h1> <p class="post-description"></p> </header> <article> <p>The workshop will host two challenges on tasks that are crucial to enable real-world vision-based assistants. These challenges are designed to test both the low-level visual capabilities and higher-level reasoning skills of vision-based assistants.</p> <ul> <li> <font color="blue"><b>The winning teams will receive a prize and are invited to present their solution in a contributed talk.</b></font> </li> <li> <font color="red"><b>Deadline: June 6, 2025</b></font> </li> </ul> <p><br></p> <h3 id="challenge-1-interactive-feedback-generation">Challenge 1: Interactive Feedback Generation</h3> <p><img src="/2025/assets/img/teaser_var_competition_1.jpeg" alt="" style="margin:auto; display:block;width:80%"></p> <p>This challenge focuses on assisting users through a workout session with interactive feedback. Details:</p> <ul> <li> <p><strong>Evaluation Data:</strong> We base this challenge on the <a href="https://www.qualcomm.com/developer/software/qevd-dataset" rel="external nofollow noopener" target="_blank">QEVD</a> dataset, as described <a href="https://arxiv.org/abs/2407.08101" rel="external nofollow noopener" target="_blank">here</a>. Specifically, the challenge involves providing timed feedback for a set of evaluation videos. For this challenge, we employ a (private) test set available <a href="https://softwarecenter.qualcomm.com/api/download/software/dataset/AIDataset/Qualcomm_Exercise_Video_Dataset/QEVD-FIT-COACH-Competition-CVPR2025/QEVD-FIT-COACH-Competition-CVPR2025.zip" rel="external nofollow noopener" target="_blank">here</a>.</p> </li> <li> <p><strong>Training and Validation Data:</strong> For training and validation, please use the data provided in the <a href="https://www.qualcomm.com/developer/software/qevd-dataset" rel="external nofollow noopener" target="_blank">QEVD page</a>.</p> </li> <li> <p><strong>Evaluation Metrics:</strong> We will use the METEOR, ROUGE-L, BERT, LLM-Acc., and T-F-Score as described <a href="https://arxiv.org/abs/2407.08101" rel="external nofollow noopener" target="_blank">here</a>. The code for these metrics is available <a href="https://github.com/Qualcomm-AI-research/FitCoach/tree/main/scripts" rel="external nofollow noopener" target="_blank">here</a>. If you have any questions contact us <a href="mailto:var.workshop.cvpr@gmail.com">here</a>.</p> </li> <li> <p><strong>Participation:</strong></p> <ul> <li> <strong>Leaderboard:</strong> Please email the results <a href="mailto:var.workshop.cvpr@gmail.com">here</a> as a json file along with the team name. The json file should contain a list of python dicts with the the following fields: <div class="language-plaintext highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>  [{“video_file”: &lt;str: name of the evaluation video file&gt;,
  “feedbacks”: &lt;List[str]: list of predicted feedbacks&gt;,
  “feedback_timestamps”: &lt;List[float]: list of timestamps corresponding to the predicted feedbacks&gt;}, ...]
</code></pre></div> </div> <p>Each team will be allowed to make five submissions and we will provide the evaluation results of each submission as soon as possible.</p> </li> <li> <p><strong>Extended Abstract:</strong> The teams submitting to the challenge are also encouraged to submit an extended abstract through <a href="http://cmt3.research.microsoft.com/VAR2025/" rel="external nofollow noopener" target="_blank">CMT</a>. The page limit is a minimum of two pages and a maximum of four pages, excluding references. As subject area please choose “Challenge -&gt; Interactive Feedback Generation”.</p> </li> <li> <strong>Winner:</strong> The winning team will be decided based on the five evaluation metrics described above. The winning team is the one that outperforms others on most metrics. The code of the winning team will be inspected before the workshop. The winner will receive a prize along with a contributed talk at the workshop.</li> </ul> </li> </ul> <p><strong>Results: Open Source Models</strong></p> <table class="table-bordered"> <thead> <tr> <th style="text-align: left">Method</th> <th style="text-align: left">METEOR↑</th> <th style="text-align: left">ROUGE-L↑</th> <th style="text-align: left">BERT↑</th> <th style="text-align: left">LLM-Acc↑</th> <th style="text-align: left">T-F-Score↑</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">VideoChat2</td> <td style="text-align: left">0.104</td> <td style="text-align: left">0.048</td> <td style="text-align: left">0.846</td> <td style="text-align: left">2.145</td> <td style="text-align: left">0.555</td> </tr> <tr> <td style="text-align: left">VideoLLaMA3-7B</td> <td style="text-align: left">0.150</td> <td style="text-align: left">0.076</td> <td style="text-align: left">0.859</td> <td style="text-align: left">2.554</td> <td style="text-align: left">0.555</td> </tr> <tr> <td style="text-align: left">Qwen-2-VL-Instruct</td> <td style="text-align: left"><strong>0.185</strong></td> <td style="text-align: left"><strong>0.089</strong></td> <td style="text-align: left"><strong>0.861</strong></td> <td style="text-align: left">2.851</td> <td style="text-align: left">0.555</td> </tr> <tr> <td style="text-align: left">Qwen-2.5-VL-Instruct</td> <td style="text-align: left">0.174</td> <td style="text-align: left">0.068</td> <td style="text-align: left">0.855</td> <td style="text-align: left"><strong>3.153</strong></td> <td style="text-align: left">0.555</td> </tr> </tbody> </table> <p><br><strong>Results: Best Challenge Submission</strong></p> <table class="table-bordered"> <thead> <tr> <th style="text-align: left">Team</th> <th style="text-align: left">METEOR↑</th> <th style="text-align: left">ROUGE-L↑</th> <th style="text-align: left">BERT↑</th> <th style="text-align: left">LLM-Acc↑</th> <th style="text-align: left">T-F-Score↑</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><a href="https://lucasventura.com/" rel="external nofollow noopener" target="_blank">Lucas Ventura</a></td> <td style="text-align: left">0.156</td> <td style="text-align: left"><strong>0.101</strong></td> <td style="text-align: left"><strong>0.861</strong></td> <td style="text-align: left">2.087</td> <td style="text-align: left">0.535</td> </tr> </tbody> </table> <p><br></p> <h3 id="challenge-2-interactive-question-answering">Challenge 2: Interactive Question Answering</h3> <p><img src="/2025/assets/img/teaser_var_competition_2.jpeg" alt="" style="margin:auto; display:block;width:100%"></p> <p>This challenge tests the ability of vision-based assistants to converse face-to-face with a human user. This challenge requires models not only to have robust audio-visual perception but also requires reasoning about scenes and events that are unfolding live in front of the camera. Details:</p> <ul> <li> <p><strong>Evaluation Data:</strong> We base this challenge on the QIVD dataset, as described <a href="https://arxiv.org/pdf/2503.19356" rel="external nofollow noopener" target="_blank">here</a>. Specifically, the challenge involves providing (timely) answers to question posed by users in the videos <a href="https://softwarecenter.qualcomm.com/api/download/software/dataset/AIDataset/QIVD/videos.zip" rel="external nofollow noopener" target="_blank">here</a>. The evaluation will be performed based on predicted answers and the associated timestamps. Note that, the questions are not provided and thus need to be infered from the audio associated with each video.</p> </li> <li> <p><strong>Training and Validation Data:</strong> We provide a set of in-context examples <a href="https://softwarecenter.qualcomm.com/api/download/software/dataset/AIDataset/QIVD/annotations_contest.zip" rel="external nofollow noopener" target="_blank">here</a>.</p> </li> <li> <p><strong>Evaluation Metrics:</strong> We will use the Correctness, BERT, METEOR, BLEU and ROUGE-L scores described <a href="https://arxiv.org/pdf/2503.19356" rel="external nofollow noopener" target="_blank">here</a>. If you have any questions contact us <a href="mailto:var.workshop.cvpr@gmail.com">here</a>.</p> </li> <li> <p><strong>Participation:</strong></p> <ul> <li> <strong>Leaderboard:</strong> Please email the results <a href="mailto:var.workshop.cvpr@gmail.com">here</a> as a json file along with the team name. The json file should contain a list of python dicts with the the following fields (see <a href="https://softwarecenter.qualcomm.com/api/download/software/dataset/AIDataset/QIVD/annotations_contest.zip" rel="external nofollow noopener" target="_blank">here</a> for an example): <div class="language-plaintext highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>  [{“video”: &lt;str: name of the evaluation video file&gt;,
  “answer”: &lt;str: the predicted answet&gt;,
  “timestamp”: &lt;float: the predicted timestamp assoicated with the answer&gt;}, ...]
</code></pre></div> </div> <p>Each team will be allowed to make five submissions and we will provide the evaluation results of each submission as soon as possible.</p> </li> <li> <p><strong>Extended Abstract:</strong> The teams submitting to the challenge are also encouraged to submit an extended abstract through <a href="http://cmt3.research.microsoft.com/VAR2025/" rel="external nofollow noopener" target="_blank">CMT</a>. The page limit is a minimum of two pages and a maximum of four pages, excluding references. As subject area please choose “Challenge -&gt; Interactive Question Answering”.</p> </li> <li> <strong>Winner:</strong> The winning team will be decided based on the five evaluation metrics described above. The winning team is the one that outperforms others on most metrics. The code of the winning team will be inspected before the workshop. The winner will receive a prize along with a contributed talk at the workshop.</li> </ul> </li> </ul> <p><strong>Results: Open Source Models</strong></p> <table class="table-bordered"> <thead> <tr> <th style="text-align: left">Method</th> <th style="text-align: left">Corr↑</th> <th style="text-align: left">BERT↑</th> <th style="text-align: left">METEOR↑</th> <th style="text-align: left">BLEU↑</th> <th style="text-align: left">ROUGE-L↑</th> <th style="text-align: left">T-Diff↓</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">VideoLLaMA</td> <td style="text-align: left">33.52</td> <td style="text-align: left">89.50</td> <td style="text-align: left">39.06</td> <td style="text-align: left">7.62</td> <td style="text-align: left">30.84</td> <td style="text-align: left">1.14</td> </tr> <tr> <td style="text-align: left">VideoLLaMA2-7B</td> <td style="text-align: left">44.31</td> <td style="text-align: left">91.18</td> <td style="text-align: left"><strong>47.20</strong></td> <td style="text-align: left">13.93</td> <td style="text-align: left">40.63</td> <td style="text-align: left">1.14</td> </tr> <tr> <td style="text-align: left">VideoLLaMA2-72B</td> <td style="text-align: left">47.69</td> <td style="text-align: left"><strong>91.42</strong></td> <td style="text-align: left">46.58</td> <td style="text-align: left"><strong>14.03</strong></td> <td style="text-align: left"><strong>41.70</strong></td> <td style="text-align: left">1.14</td> </tr> <tr> <td style="text-align: left">VideoLLaMA3-7B</td> <td style="text-align: left">52.31</td> <td style="text-align: left">90.92</td> <td style="text-align: left">45.20</td> <td style="text-align: left">11.21</td> <td style="text-align: left">40.54</td> <td style="text-align: left">1.14</td> </tr> <tr> <td style="text-align: left">Qwen2.5-VL-7B</td> <td style="text-align: left"><strong>53.55</strong></td> <td style="text-align: left">87.17</td> <td style="text-align: left">34.95</td> <td style="text-align: left">3.88</td> <td style="text-align: left">26.52</td> <td style="text-align: left">1.14</td> </tr> </tbody> </table> <p><br><strong>Results: Best Challenge Submission</strong></p> <table class="table-bordered"> <thead> <tr> <th style="text-align: left">Team</th> <th style="text-align: left">Corr↑</th> <th style="text-align: left">BERT↑</th> <th style="text-align: left">METEOR↑</th> <th style="text-align: left">BLEU↑</th> <th style="text-align: left">ROUGE-L↑</th> <th style="text-align: left">T-Diff↓</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">HNU-VPAI*</td> <td style="text-align: left">54.27</td> <td style="text-align: left">92.49</td> <td style="text-align: left">48.24</td> <td style="text-align: left">14.61</td> <td style="text-align: left">45.20</td> <td style="text-align: left">1.09</td> </tr> </tbody> </table> <p>*Zhiyu Wang, Puhong Duan, Wang Liu, Bin Sun, Xudong Kang, Shutao Li — Hunan University, Weikang Yu — Helmholtz-Zentrum Dresden-Rossendorf (HZDR) and Technical University of Munich (TUM)</p> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 VAR . Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/2025/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/2025/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/2025/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/2025/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/2025/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/2025/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/2025/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/2025/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/2025/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/2025/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>