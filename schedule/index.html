<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Program and Papers | Vision-based Assistants in the Real-World </title> <meta name="author" content="VAR "> <meta name="description" content="VAR Workshop @ CVPR 2025. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/2025/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/2025/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/2025/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/2025/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2025/assets/img/icon.png?4c4bbc383b800ef80954ff10456a6a06"> <link rel="stylesheet" href="/2025/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://varworkshop.github.io/2025/schedule/"> <script src="/2025/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/2025/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2025/"> Vision-based Assistants in the Real-World </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2025/">Home </a> </li> <li class="nav-item active"> <a class="nav-link" href="/2025/schedule/">Program and Papers <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/2025/calls/">Calls </a> </li> <li class="nav-item "> <a class="nav-link" href="/2025/challenges/">Challenges </a> </li> <li class="nav-item "> <a class="nav-link" href="/2025/organizers/">Organizers </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Program and Papers</h1> <p class="post-description"></p> </header> <article> <h3 id="location-and-schedule">Location and Schedule</h3> <ul> <li> <strong>Date:</strong> June 11, 2025.</li> <li> <strong>Workshop/Demo Location:</strong> Room 211 (Level 2). Table available for demo setup.</li> <li> <strong>Poster Location:</strong> Poster boards #205 - #216, ExHall D. Organizers will be in ExHall D to assist.</li> </ul> <table class="table-bordered"> <thead> <tr> <th style="text-align: left">Time Slot</th> <th style="text-align: left">Event</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">13:30-13:45</td> <td style="text-align: left">Welcome and Introduction</td> </tr> <tr> <td style="text-align: left">13:45-14:15</td> <td style="text-align: left">Keynote: Mike Shou</td> </tr> <tr> <td style="text-align: left">14:15-14:45</td> <td style="text-align: left">Keynote: Kate Saenko</td> </tr> <tr> <td style="text-align: left">14:45-15:30</td> <td style="text-align: left">Posters, Demos and Coffee</td> </tr> <tr> <td style="text-align: left">15:30-16:00</td> <td style="text-align: left">Keynote: Ani Kembhavi</td> </tr> <tr> <td style="text-align: left">16:00-16:30</td> <td style="text-align: left">Keynote: Marc Pollefeys</td> </tr> <tr> <td style="text-align: left">16:30-17:00</td> <td style="text-align: left">Challenge Results</td> </tr> <tr> <td style="text-align: left">17:00-17:50</td> <td style="text-align: left">Panel Discussion</td> </tr> <tr> <td style="text-align: left">17:50-18:00</td> <td style="text-align: left">Closing Remarks</td> </tr> </tbody> </table> <p><br>All times are in local time zone, Central Daylight Time (CDT). <br></p> <h3 id="accepted-demos">Accepted Demos</h3> <p><strong>Vision-Language Guided Object Localization in Mixed Reality</strong> <br> <em>Han Xi, Ard Kastrati, Dushan Vasilevski, Roger Wattenhofer</em><br></p> <p><strong>LiveCC: Learning Video LLM with Streaming Speech Transcription at Scale</strong> <br> <em>Joya Chen, Ziyun Zeng, Yiqi Lin, Wei Li, Zejun Ma, Mike Zheng Shou</em><br></p> <p><br></p> <h3 id="accepted-papers-extended-abstracts">Accepted Papers: Extended Abstracts</h3> <p><strong>HandsOnVLM: Vision-Language Models for Hand-Object Interaction Prediction</strong> <br> <em>Chen Bao, Jiarui Xu, Xiaolong Wang, Abhinav Gupta, Homanga Bharadhwaj</em><br> <a href="/assets/pdf/8.pdf">pdf</a> | <a href="https://github.com/Kami-code/HandsOnVLM-release" rel="external nofollow noopener" target="_blank">code</a> | <a href="https://www.chenbao.tech/handsonvlm/" rel="external nofollow noopener" target="_blank">video</a></p> <p><strong>Learning to Perceive and Act: Active Event Understanding via Predictive Free Energy Minimization</strong> <br> <em>Zhou Chen, Sanjoy Kundun, Harsimran Baweja, Sathyanarayanan Aakur</em><br> <a href="/assets/pdf/9.pdf">pdf</a></p> <p><strong>InteractFormer: Modeling Agent Interactions for Multi-Agent Action Anticipation</strong> <br> <em>Yiqi Jin, Simon Stepputtis, Katia Sycara, Yaqi Xie</em><br> <a href="/assets/pdf/12.pdf">pdf</a></p> <p><strong>Vision-Language Guided Object Localization in Mixed Reality</strong> <br> <em>Han Xi, Ard Kastrati, Dushan Vasilevski, Roger Wattenhofer</em><br> <a href="/assets/pdf/15.pdf">pdf</a> | <a href="https://youtu.be/XmQUBR-jZgQ" rel="external nofollow noopener" target="_blank">video</a></p> <p><strong>Vid2Coach: Transforming How-To Videos into Task Assistants</strong> <br> <em>Mina Huh, Zihui Xue, Ujjaini Das, Kumar Ashutosh, Kristen Grauman, Amy Pavel</em><br> <a href="/assets/pdf/17.pdf">pdf</a> | <a href="https://arxiv.org/abs/2506.00717" rel="external nofollow noopener" target="_blank">arXiv</a> | <a href="https://www.youtube.com/watch?v=M0j2TmezItI" rel="external nofollow noopener" target="_blank">video</a></p> <p><strong>Plan-Action-Reflection: A Three-Role Agentic Framework For Computer Use Agent Task</strong> <br> <em>Xin Su, Man Luo, David Cobbley, Shachar Rosenman, Vasudev Lal, Phillip Howard</em><br> <a href="/assets/pdf/18.pdf">pdf</a></p> <p><br></p> <h3 id="accepted-papers-cvpr-full-papers">Accepted Papers: CVPR Full Papers</h3> <p><strong>Gazing Into Missteps: Leveraging Eye-Gaze for Unsupervised Mistake Detection in Egocentric Videos of Skilled Human Activities</strong> <br> <em>Michele Mazzamuto</em><br> <a href="/assets/pdf/1.pdf">pdf</a></p> <p><strong>DIV-FF: Dynamic Image-Video Feature Fields For Environment Understanding in Egocentric Videos</strong> <br> <em>Lorenzo Mur-Labadia, Ruben Martinez-Cantin, Josechu Guerrero</em><br> <a href="/assets/pdf/2.pdf">pdf</a> | <a href="https://github.com/lmur98/DIV_FF_CVPR" rel="external nofollow noopener" target="_blank">code</a></p> <p><strong>LongVALE: Vision-Audio-Language-Event Benchmark Towards Time-Aware Omni-Modal Perception of Long Videos</strong> <br> <em>TianTian Geng, Jinrui Zhang, Qingni Wang, Teng Wang, Jinming Duan, Feng Zheng</em><br> <a href="/assets/pdf/4.pdf">pdf</a></p> <p><strong>EgoTextVQA: Towards Egocentric Scene-Text Aware Video Question Answering</strong> <br> <em>Sheng Zhou, Junbin Xiao, Qingyun Li, Yicong Li, Xun Yang, Dan Guo, Meng Wang, Tat-Seng Chua, Angela Yao</em><br> <a href="/assets/pdf/5.pdf">pdf</a></p> <p><strong>MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based Scientific Research</strong> <br> <em>James Burgess, Jeffrey Nirschl, Laura Bravo-Sánchez, Alejandro Lozano, Sanket Gupte, Jesus Galaz-Montoya, Yuhui Zhang, Yuchang Su, Disha Bhowmik, Zachary Coman, Sarina Hasan, Alexandra Johannesson, William Leineweber, Malvika Nair, Ridhi Yarlagadda, Connor Zuraski, Wah Chiu, Sarah Cohen, Jan Hansen, Manuel Leonetti, Chad Liu, Emma Lundberg, Serena Yeung-Levy</em><br> <a href="/assets/pdf/6.pdf">pdf</a></p> <p><strong>Not Only Text: Exploring Compositionality of Visual Representations in Vision-Language Models</strong> <br> <em>Davide Berasi, Matteo Farina, Massimiliano Mancini, Elisa Ricci, Nicola Strisciuglio</em><br> <a href="/assets/pdf/7.pdf">pdf</a></p> <p><strong>LiveCC: Learning Video LLM with Streaming Speech Transcription at Scale</strong> <br> <em>Joya Chen, Ziyun Zeng, Yiqi Lin, Wei Li, Zejun Ma, Mike Zheng Shou</em><br> <a href="/assets/pdf/10.pdf">pdf</a> | <a href="https://github.com/showlab/livecc" rel="external nofollow noopener" target="_blank">code</a> | <a href="https://showlab.github.io/livecc/" rel="external nofollow noopener" target="_blank">video</a></p> <p><strong>Vision-Language Models Do Not Understand Negation</strong> <br> <em>Kumail Alhamoud, Shaden Alshammari, Yonglong Tian, Guohao Li, Philip Torr, Yoon Kim, Marzyeh Ghassemi</em><br> <a href="/assets/pdf/13.pdf">pdf</a> | <a href="https://github.com/m1k2zoo/negbench" rel="external nofollow noopener" target="_blank">code</a> | <a href="https://www.youtube.com/watch?v=oOJxwlKUE8M" rel="external nofollow noopener" target="_blank">video</a></p> <p><strong>VisionZip: Longer is Better but Not Necessary in Vision Language Models</strong> <br> <em>Senqiao Yang</em><br> <a href="/assets/pdf/14.pdf">pdf</a> | <a href="https://github.com/dvlab-research/VisionZip" rel="external nofollow noopener" target="_blank">code</a> | <a href="https://youtu.be/sytaAzmxxpo?si=IieArmQ7YNf2dVyM" rel="external nofollow noopener" target="_blank">video</a></p> <p><strong>Which Viewpoint Shows it Best? Language for Weakly Supervising View Selection in Multi-view Instructional Videos</strong> <br> <em>Sagnik Majumder, Tushar Nagarajan, Ziad Al-Halah, Reina Pradhan, Kristen Grauman</em><br> <a href="/assets/pdf/16.pdf">pdf</a></p> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 VAR . Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/2025/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/2025/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/2025/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/2025/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/2025/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/2025/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/2025/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/2025/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/2025/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/2025/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>