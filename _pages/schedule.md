---
layout: page
permalink: /schedule/
title: Program and Papers
description: 
nav: true
nav_order: 2
---

### Location and Schedule
- **Date:** June 11, 2025.
- **Workshop/Demo Location:** Room 211 (Level 2). Table available for demo setup.
- **Poster Location:** Poster boards #205 - #216,  ExHall D. Organizers will be in ExHall D to assist.

| Time Slot    | Event                      |
| :---------- | :------------------------- |
| 13:30-13:45 | Welcome and Introduction    |
| 13:45-14:15 | Keynote: Mike Shou         |
| 14:15-14:45 | Keynote: Kate Saenko       |
| 14:45-15:30 | Posters, Demos and Coffee |
| 15:30-16:00 | Keynote: Ani Kembhavi     |
| 16:00-16:30 | Keynote: Marc Pollefeys   |
| 16:30-17:00 | Challenge Results          |
| 17:00-17:50 | Panel Discussion            |
| 17:50-18:00 | Closing Remarks             |
{:.table-bordered}


<br/>All times are in local time zone, Central Daylight Time (CDT).
<br/>

### Accepted Demos
**Vision-Language Guided Object Localization in Mixed Reality**
<br/>
*Han Xi, Ard Kastrati, Dushan Vasilevski, Roger Wattenhofer*<br/>

**LiveCC: Learning Video LLM with Streaming Speech Transcription at Scale**
<br/>
*Joya Chen, Ziyun Zeng, Yiqi Lin, Wei Li, Zejun Ma, Mike Zheng Shou*<br/>

<br/>

### Accepted Papers: Extended Abstracts
**HandsOnVLM: Vision-Language Models for Hand-Object Interaction Prediction**
<br/>
*Chen Bao, Jiarui Xu, Xiaolong Wang, Abhinav Gupta, Homanga Bharadhwaj*<br/>
[pdf](/assets/pdf/8.pdf) | [code](https://github.com/Kami-code/HandsOnVLM-release) | [video](https://www.chenbao.tech/handsonvlm/)

**Learning to Perceive and Act: Active Event Understanding via Predictive Free Energy Minimization**
<br/>
*Zhou Chen, Sanjoy Kundun, Harsimran Baweja, Sathyanarayanan Aakur*<br/>
[pdf](/assets/pdf/9.pdf)

**InteractFormer: Modeling Agent Interactions for Multi-Agent Action Anticipation**
<br/>
*Yiqi Jin, Simon Stepputtis, Katia Sycara, Yaqi Xie*<br/>
[pdf](/assets/pdf/12.pdf)

**Vision-Language Guided Object Localization in Mixed Reality**
<br/>
*Han Xi, Ard Kastrati, Dushan Vasilevski, Roger Wattenhofer*<br/>
[pdf](/assets/pdf/15.pdf) | [video](https://youtu.be/XmQUBR-jZgQ)

**Vid2Coach: Transforming How-To Videos into Task Assistants**
<br/>
*Mina Huh, Zihui Xue, Ujjaini Das, Kumar Ashutosh, Kristen Grauman, Amy Pavel*<br/>
[pdf](/assets/pdf/17.pdf) | [arXiv](https://arxiv.org/abs/2506.00717) | [video](https://www.youtube.com/watch?v=M0j2TmezItI)

**Plan-Action-Reflection: A Three-Role Agentic Framework For Computer Use Agent Task**
<br/>
*Xin Su, Man Luo, David Cobbley, Shachar Rosenman, Vasudev Lal, Phillip Howard*<br/>
[pdf](/assets/pdf/18.pdf)


<br/>

### Accepted Papers: CVPR Full Papers
**Gazing Into Missteps: Leveraging Eye-Gaze for Unsupervised Mistake Detection in Egocentric Videos of Skilled Human Activities**
<br/>
*Michele Mazzamuto*<br/>
[pdf](/assets/pdf/1.pdf)

**DIV-FF: Dynamic Image-Video Feature Fields For Environment Understanding in Egocentric Videos**
<br/>
*Lorenzo Mur-Labadia, Ruben Martinez-Cantin, Josechu Guerrero*<br/>
[pdf](/assets/pdf/2.pdf) | [code](https://github.com/lmur98/DIV_FF_CVPR)

**LongVALE: Vision-Audio-Language-Event Benchmark Towards Time-Aware Omni-Modal Perception of Long Videos**
<br/>
*TianTian Geng, Jinrui Zhang, Qingni Wang, Teng Wang, Jinming Duan, Feng Zheng*<br/>
[pdf](/assets/pdf/4.pdf)

**EgoTextVQA: Towards Egocentric Scene-Text Aware Video Question Answering**
<br/>
*Sheng Zhou, Junbin Xiao, Qingyun Li, Yicong Li, Xun Yang, Dan Guo, Meng Wang, Tat-Seng Chua, Angela Yao*<br/>
[pdf](/assets/pdf/5.pdf)

**MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based Scientific Research**
<br/>
*James Burgess, Jeffrey Nirschl, Laura Bravo-SÃ¡nchez, Alejandro Lozano, Sanket Gupte, Jesus Galaz-Montoya, Yuhui Zhang, Yuchang Su, Disha Bhowmik, Zachary Coman,
Sarina Hasan, Alexandra Johannesson, William Leineweber, Malvika Nair, Ridhi Yarlagadda, Connor Zuraski, Wah Chiu, Sarah Cohen, Jan Hansen, Manuel Leonetti, Chad Liu,
Emma Lundberg, Serena Yeung-Levy*<br/>
[pdf](/assets/pdf/6.pdf)

**Not Only Text: Exploring Compositionality of Visual Representations in Vision-Language Models**
<br/>
*Davide Berasi, Matteo Farina, Massimiliano Mancini, Elisa Ricci, Nicola Strisciuglio*<br/>
[pdf](/assets/pdf/7.pdf)

**LiveCC: Learning Video LLM with Streaming Speech Transcription at Scale**
<br/>
*Joya Chen, Ziyun Zeng, Yiqi Lin, Wei Li, Zejun Ma, Mike Zheng Shou*<br/>
[pdf](/assets/pdf/10.pdf) | [code](https://github.com/showlab/livecc) | [video](https://showlab.github.io/livecc/)

**Vision-Language Models Do Not Understand Negation**
<br/>
*Kumail Alhamoud, Shaden Alshammari, Yonglong Tian, Guohao Li, Philip Torr, Yoon Kim, Marzyeh Ghassemi*<br/>
[pdf](/assets/pdf/13.pdf) | [code](https://github.com/m1k2zoo/negbench) | [video](https://www.youtube.com/watch?v=oOJxwlKUE8M)

**VisionZip: Longer is Better but Not Necessary in Vision Language Models**
<br/>
*Senqiao Yang*<br/>
[pdf](/assets/pdf/14.pdf) | [code](https://github.com/dvlab-research/VisionZip) | [video](https://youtu.be/sytaAzmxxpo?si=IieArmQ7YNf2dVyM)

**Which Viewpoint Shows it Best? Language for Weakly Supervising View Selection in Multi-view Instructional Videos**
<br/>
*Sagnik Majumder, Tushar Nagarajan, Ziad Al-Halah, Reina Pradhan, Kristen Grauman*<br/>
[pdf](/assets/pdf/16.pdf)
